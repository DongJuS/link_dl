c:\Users\didsu\workspace\deeplearning\link_dl\_04_your_code\homework_2\titanic_dataset.py:129: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  all_df["alone"].fillna(0, inplace=True)
c:\Users\didsu\workspace\deeplearning\link_dl\_04_your_code\homework_2\titanic_dataset.py:148: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  all_df["Embarked"].fillna("missing", inplace=True)
Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',
       'Embarked', 'title', 'family_num', 'alone'],
      dtype='object')
   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  title  \
0       0.0       3    1  22.0      1      0   7.2500         2      2
1       1.0       1    0  38.0      1      0  71.2833         0      3
2       1.0       3    0  26.0      0      0   7.9250         2      1
3       1.0       1    0  35.0      1      0  53.1000         2      3
4       0.0       3    1  35.0      0      0   8.0500         2      2
5       0.0       3    1  29.0      0      0   8.4583         1      2
6       0.0       1    1  54.0      0      0  51.8625         2      2
7       0.0       3    1   2.0      3      1  21.0750         2      0
8       1.0       3    0  27.0      0      2  11.1333         2      3
9       1.0       2    0  14.0      1      0  30.0708         0      3

   family_num  alone
0           1    0.0
1           1    0.0
2           0    1.0
3           1    0.0
4           0    1.0
5           0    1.0
6           0    1.0
7           4    0.0
8           2    0.0
9           1    0.0
Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])
Train: 713, Validation: 178, Test: 418
Epoch 100, Training loss 0.6291, Validation loss 0.6051, Patience: 0/100
Epoch 200, Training loss 0.6132, Validation loss 0.5958, Patience: 0/100
Epoch 300, Training loss 0.6028, Validation loss 0.5902, Patience: 0/100
Epoch 400, Training loss 0.5950, Validation loss 0.5861, Patience: 0/100
Epoch 500, Training loss 0.5943, Validation loss 0.5840, Patience: 3/100
Epoch 600, Training loss 0.5902, Validation loss 0.5811, Patience: 0/100
Epoch 700, Training loss 0.5837, Validation loss 0.5790, Patience: 4/100
Epoch 800, Training loss 0.5830, Validation loss 0.5766, Patience: 0/100
Epoch 900, Training loss 0.5770, Validation loss 0.5747, Patience: 2/100
Epoch 1000, Training loss 0.5661, Validation loss 0.5719, Patience: 4/100
Epoch 1100, Training loss 0.5709, Validation loss 0.5697, Patience: 2/100
Epoch 1200, Training loss 0.5687, Validation loss 0.5670, Patience: 1/100
Epoch 1300, Training loss 0.5594, Validation loss 0.5645, Patience: 0/100
Epoch 1400, Training loss 0.5603, Validation loss 0.5642, Patience: 2/100
Epoch 1500, Training loss 0.5631, Validation loss 0.5630, Patience: 1/100
Epoch 1600, Training loss 0.5590, Validation loss 0.5600, Patience: 3/100
Epoch 1700, Training loss 0.5501, Validation loss 0.5557, Patience: 2/100
Epoch 1800, Training loss 0.5524, Validation loss 0.5523, Patience: 1/100
Epoch 1900, Training loss 0.5462, Validation loss 0.5485, Patience: 1/100
Epoch 2000, Training loss 0.5415, Validation loss 0.5456, Patience: 6/100
Epoch 2100, Training loss 0.5312, Validation loss 0.5418, Patience: 0/100
Epoch 2200, Training loss 0.5390, Validation loss 0.5454, Patience: 3/100
Epoch 2300, Training loss 0.5320, Validation loss 0.5378, Patience: 1/100
Epoch 2400, Training loss 0.5224, Validation loss 0.5331, Patience: 9/100
Epoch 2500, Training loss 0.5209, Validation loss 0.5294, Patience: 1/100
Epoch 2600, Training loss 0.5160, Validation loss 0.5216, Patience: 4/100
Epoch 2700, Training loss 0.5094, Validation loss 0.5165, Patience: 0/100
Epoch 2800, Training loss 0.5010, Validation loss 0.5108, Patience: 1/100
Epoch 2900, Training loss 0.5009, Validation loss 0.5259, Patience: 5/100
Epoch 3000, Training loss 0.4921, Validation loss 0.5029, Patience: 4/100
Epoch 3100, Training loss 0.4894, Validation loss 0.5094, Patience: 3/100
Epoch 3200, Training loss 0.4818, Validation loss 0.4923, Patience: 9/100
Epoch 3300, Training loss 0.4840, Validation loss 0.4942, Patience: 3/100
Epoch 3400, Training loss 0.4730, Validation loss 0.4957, Patience: 3/100
Epoch 3500, Training loss 0.4589, Validation loss 0.4768, Patience: 1/100
Epoch 3600, Training loss 0.4559, Validation loss 0.4823, Patience: 1/100
Epoch 3700, Training loss 0.4527, Validation loss 0.5081, Patience: 2/100
Epoch 3800, Training loss 0.4410, Validation loss 0.4642, Patience: 0/100
Epoch 3900, Training loss 0.4549, Validation loss 0.4647, Patience: 9/100
Epoch 4000, Training loss 0.4377, Validation loss 0.4592, Patience: 5/100
Epoch 4100, Training loss 0.4414, Validation loss 0.4561, Patience: 2/100
Epoch 4200, Training loss 0.4441, Validation loss 0.5485, Patience: 34/100
Epoch 4300, Training loss 0.4410, Validation loss 0.4662, Patience: 7/100
Epoch 4400, Training loss 0.4399, Validation loss 0.4541, Patience: 9/100
Epoch 4500, Training loss 0.4379, Validation loss 0.4498, Patience: 16/100
Epoch 4600, Training loss 0.4341, Validation loss 0.4582, Patience: 16/100
Epoch 4700, Training loss 0.4304, Validation loss 0.4493, Patience: 17/100
Epoch 4800, Training loss 0.4622, Validation loss 0.4460, Patience: 62/100
Epoch 4900, Training loss 0.4394, Validation loss 0.4581, Patience: 31/100
Epoch 5000, Training loss 0.4297, Validation loss 0.4543, Patience: 4/100

요구사항 2 완료! Best Validation Loss: 0.4395
Wandb URL: https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2/runs/puqzu1vq
