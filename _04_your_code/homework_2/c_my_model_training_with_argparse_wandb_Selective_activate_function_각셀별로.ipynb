{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04e3c89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d888b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didsu\\miniconda3\\envs\\link_dl\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didsu\\miniconda3\\envs\\link_dl\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m-ddj127\u001b[0m (\u001b[33m-ddj127-korea-university-of-technology-and-education\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# wandb 로그인\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22de956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: c:\\Users\\didsu\\workspace\\deeplearning\\link_dl\\_03_homeworks\\homework_2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\didsu\\workspace\\deeplearning\\link_dl\\_03_homeworks\\homework_2\\wandb\\run-20251017_173818-nf1ilk6c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/my_model_training/runs/nf1ilk6c' target=\"_blank\">ReLU_b128_early</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/my_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/my_model_training/runs/nf1ilk6c' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/my_model_training/runs/nf1ilk6c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x0000020D188A93C0>\n",
      "{'epochs': 5000, 'batch_size': 128, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20], 'activation_fn': 'ReLU', 'patience': 100}\n",
      "Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
      "       'Embarked', 'title', 'family_num', 'alone'],\n",
      "      dtype='object')\n",
      "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  title  \\\n",
      "0       0.0       3    1  22.0      1      0   7.2500         2      2   \n",
      "1       1.0       1    0  38.0      1      0  71.2833         0      3   \n",
      "2       1.0       3    0  26.0      0      0   7.9250         2      1   \n",
      "3       1.0       1    0  35.0      1      0  53.1000         2      3   \n",
      "4       0.0       3    1  35.0      0      0   8.0500         2      2   \n",
      "5       0.0       3    1  29.0      0      0   8.4583         1      2   \n",
      "6       0.0       1    1  54.0      0      0  51.8625         2      2   \n",
      "7       0.0       3    1   2.0      3      1  21.0750         2      0   \n",
      "8       1.0       3    0  27.0      0      2  11.1333         2      3   \n",
      "9       1.0       2    0  14.0      1      0  30.0708         0      3   \n",
      "\n",
      "   family_num  alone  \n",
      "0           1    0.0  \n",
      "1           1    0.0  \n",
      "2           0    1.0  \n",
      "3           1    0.0  \n",
      "4           0    1.0  \n",
      "5           0    1.0  \n",
      "6           0    1.0  \n",
      "7           4    0.0  \n",
      "8           2    0.0  \n",
      "9           1    0.0  \n",
      "Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])\n",
      "713 178 418\n",
      "################################################## 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didsu\\workspace\\deeplearning\\link_dl\\_03_homeworks\\homework_2\\titanic_dataset.py:129: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "c:\\Users\\didsu\\workspace\\deeplearning\\link_dl\\_03_homeworks\\homework_2\\titanic_dataset.py:148: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.5960, Validation loss 0.6630Patience: 9/100\n",
      "Epoch 200, Training loss 0.5901, Validation loss 0.6588Patience: 9/100\n",
      "Epoch 300, Training loss 0.5873, Validation loss 0.6552Patience: 4/100\n",
      "Epoch 400, Training loss 0.5800, Validation loss 0.6532Patience: 22/100\n",
      "Epoch 500, Training loss 0.5841, Validation loss 0.6497Patience: 4/100\n",
      "Epoch 600, Training loss 0.5823, Validation loss 0.6483Patience: 14/100\n",
      "Epoch 700, Training loss 0.5794, Validation loss 0.6479Patience: 36/100\n",
      "Epoch 800, Training loss 0.5713, Validation loss 0.6470Patience: 12/100\n",
      "Epoch 900, Training loss 0.5740, Validation loss 0.6441Patience: 7/100\n",
      "Epoch 1000, Training loss 0.5748, Validation loss 0.6418Patience: 2/100\n",
      "Epoch 1100, Training loss 0.5691, Validation loss 0.6405Patience: 6/100\n",
      "Epoch 1200, Training loss 0.5690, Validation loss 0.6390Patience: 1/100\n",
      "Epoch 1300, Training loss 0.5655, Validation loss 0.6387Patience: 17/100\n",
      "Epoch 1400, Training loss 0.5653, Validation loss 0.6376Patience: 33/100\n",
      "Epoch 1500, Training loss 0.5665, Validation loss 0.6360Patience: 45/100\n",
      "Epoch 1600, Training loss 0.5592, Validation loss 0.6344Patience: 21/100\n",
      "Epoch 1700, Training loss 0.5586, Validation loss 0.6335Patience: 5/100\n",
      "Epoch 1800, Training loss 0.5549, Validation loss 0.6290Patience: 1/100\n",
      "Epoch 1900, Training loss 0.5592, Validation loss 0.6262Patience: 8/100\n",
      "Epoch 2000, Training loss 0.5475, Validation loss 0.6246Patience: 7/100\n",
      "Epoch 2100, Training loss 0.5538, Validation loss 0.6229Patience: 36/100\n",
      "Epoch 2200, Training loss 0.5568, Validation loss 0.6200Patience: 12/100\n",
      "Epoch 2300, Training loss 0.5501, Validation loss 0.6184Patience: 5/100\n",
      "Epoch 2400, Training loss 0.5479, Validation loss 0.6165Patience: 35/100\n",
      "Epoch 2500, Training loss 0.5415, Validation loss 0.6145Patience: 5/100\n",
      "Epoch 2600, Training loss 0.5459, Validation loss 0.6108Patience: 22/100\n",
      "Epoch 2700, Training loss 0.5380, Validation loss 0.6099Patience: 7/100\n",
      "Epoch 2800, Training loss 0.5318, Validation loss 0.6068Patience: 12/100\n",
      "Epoch 2900, Training loss 0.5362, Validation loss 0.6007Patience: 0/100\n",
      "Epoch 3000, Training loss 0.5253, Validation loss 0.5979Patience: 1/100\n",
      "Epoch 3100, Training loss 0.5262, Validation loss 0.5954Patience: 7/100\n",
      "Epoch 3200, Training loss 0.5232, Validation loss 0.5917Patience: 20/100\n",
      "Epoch 3300, Training loss 0.5217, Validation loss 0.5897Patience: 29/100\n",
      "Epoch 3400, Training loss 0.5160, Validation loss 0.5847Patience: 1/100\n",
      "Epoch 3500, Training loss 0.5101, Validation loss 0.5766Patience: 6/100\n",
      "Epoch 3600, Training loss 0.5092, Validation loss 0.5735Patience: 8/100\n",
      "Epoch 3700, Training loss 0.5040, Validation loss 0.5672Patience: 11/100\n",
      "Epoch 3800, Training loss 0.4974, Validation loss 0.5651Patience: 11/100\n",
      "Epoch 3900, Training loss 0.4979, Validation loss 0.5638Patience: 23/100\n",
      "Epoch 4000, Training loss 0.4964, Validation loss 0.5467Patience: 3/100\n",
      "Epoch 4100, Training loss 0.4846, Validation loss 0.5442Patience: 22/100\n",
      "Epoch 4200, Training loss 0.4757, Validation loss 0.5438Patience: 36/100\n",
      "Epoch 4300, Training loss 0.4721, Validation loss 0.5361Patience: 15/100\n",
      "Epoch 4400, Training loss 0.4733, Validation loss 0.5560Patience: 39/100\n",
      "Epoch 4500, Training loss 0.4737, Validation loss 0.5195Patience: 48/100\n",
      "Epoch 4600, Training loss 0.4596, Validation loss 0.5577Patience: 33/100\n",
      "Epoch 4700, Training loss 0.4644, Validation loss 0.5100Patience: 29/100\n",
      "Epoch 4800, Training loss 0.4610, Validation loss 0.5116Patience: 19/100\n",
      "Epoch 4900, Training loss 0.4521, Validation loss 0.4978Patience: 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000, Training loss 0.4565, Validation loss 0.5090Patience: 21/100\n",
      "\n",
      "##################################################\n",
      "Generating predictions for test data...\n",
      "✅ Predictions saved to submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>Training loss</td><td>██▇██▇▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▂▂▁▁▁</td></tr><tr><td>Validation loss</td><td>█████▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5000</td></tr><tr><td>Training loss</td><td>0.45649</td></tr><tr><td>Validation loss</td><td>0.50897</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ReLU_b128_early</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/my_model_training/runs/nf1ilk6c' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/my_model_training/runs/nf1ilk6c</a><br> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/my_model_training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251017_173818-nf1ilk6c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "BASE_PATH = os.getcwd()  \n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "from titanic_dataset import get_preprocessed_dataset\n",
    "\n",
    "\n",
    "def get_data():\n",
    "  train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "  print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "  train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "  validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "  test_data_loader = DataLoader(dataset= test_dataset, batch_size=len(test_dataset))\n",
    "  return train_data_loader, validation_data_loader, test_data_loader\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self, n_input, n_output, activation_fn): # 이 줄을 수정\n",
    "    super().__init__()\n",
    "    # 문자열 이름과 실제 활성화 함수 클래스를 매핑\n",
    "    activation_functions = { # 이 줄을 추가\n",
    "        'ReLU': nn.ReLU(),\n",
    "        'Sigmoid': nn.Sigmoid(),\n",
    "        'ELU': nn.ELU(),\n",
    "        'Leaky ReLU': nn.LeakyReLU()\n",
    "    }\n",
    "\n",
    "# BCEWithLogitsLoss용\n",
    "    self.model = nn.Sequential(\n",
    "      nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "      activation_functions[activation_fn], # 이 줄을 수정\n",
    "      nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "      activation_functions[activation_fn], # 이 줄을 수정\n",
    "      nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_model_and_optimizer():\n",
    "  my_model = MyModel(n_input=10, n_output=1, activation_fn=wandb.config.activation_fn) # 이 줄 수정\n",
    "  optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "  return my_model, optimizer\n",
    "\n",
    "\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader, test_data_loader): \n",
    "  n_epochs = wandb.config.epochs\n",
    "  loss_fn = nn.BCEWithLogitsLoss()\n",
    "  next_print_epoch = 100\n",
    "\n",
    "  # Early stopping 변수 추가(하단의 validation loss까지 구해진 다음에 early stopping을 할지를 결정하는 부분이라 많은 코드가 요구되지 않음)\n",
    "  best_validation_loss = float('inf')\n",
    "  patience_counter = 0\n",
    "  best_model_state = None\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss_train = 0.0\n",
    "    num_trains = 0\n",
    "    for train_batch in train_data_loader:\n",
    "      input = train_batch['input']\n",
    "      target = train_batch['target'].float().unsqueeze(1)  # Float 변환 & [512] → [512, 1]\n",
    "      output_train = model(input)\n",
    "      loss = loss_fn(output_train, target)\n",
    "      loss_train += loss.item()\n",
    "      num_trains += 1\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_validations = 0\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in validation_data_loader:\n",
    "        # input, target = validation_batch\n",
    "        input = validation_batch['input']\n",
    "        output_validation = model(input)\n",
    "        target = validation_batch['target'].float().unsqueeze(1)\n",
    "        loss = loss_fn(output_validation, target)\n",
    "        loss_validation += loss.item()\n",
    "        num_validations += 1\n",
    "\n",
    "    avg_validation_loss = loss_validation / num_validations\n",
    "\n",
    "    wandb.log({\n",
    "      \"Epoch\": epoch,\n",
    "      \"Training loss\": loss_train / num_trains,\n",
    "      \"Validation loss\": avg_validation_loss # loss_validation / num_validations\n",
    "    })\n",
    "      # Early stopping 체크\n",
    "    if avg_validation_loss < best_validation_loss:\n",
    "      best_validation_loss = avg_validation_loss\n",
    "      patience_counter = 0\n",
    "      best_model_state = model.state_dict().copy()  # 최고 모델 저장\n",
    "    else:\n",
    "      patience_counter += 1\n",
    "\n",
    "    if epoch >= next_print_epoch:\n",
    "      print(\n",
    "        f\"Epoch {epoch}, \"\n",
    "        f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "        f\"Validation loss {loss_validation / num_validations:.4f}\"\n",
    "        f\"Patience: {patience_counter}/{wandb.config.patience}\"\n",
    "      )\n",
    "      next_print_epoch += 100\n",
    "\n",
    "      # Early stopping 조건\n",
    "    if patience_counter >= wandb.config.patience:\n",
    "      print(f\"\\n Early stopping at epoch {epoch}\")\n",
    "      print(f\"Best validation loss: {best_validation_loss:.4f}\")\n",
    "      model.load_state_dict(best_model_state)  # 최고 모델 복원\n",
    "      break\n",
    "\n",
    "def predict_test(model, test_data_loader):\n",
    "  \"\"\"Test 데이터로 예측 수행\"\"\"\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for test_batch in test_data_loader:\n",
    "      input = test_batch['input']\n",
    "      output = model(input)\n",
    "      predictions.append(output)\n",
    "  \n",
    "  predictions = torch.cat(predictions, dim=0).squeeze()  # [N, 1] → [N]\n",
    "  return predictions\n",
    "\n",
    "\n",
    "def save_predictions_to_csv(predictions, filename=\"submission.csv\"):\n",
    "  \"\"\"예측 결과를 CSV로 저장 (Kaggle 제출 형식)\"\"\"\n",
    "  import pandas as pd\n",
    "  \n",
    "  # Kaggle test.csv의 PassengerId 읽기\n",
    "  CURRENT_FILE_PATH = os.getcwd()\n",
    "  test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "  test_df = pd.read_csv(test_data_path)\n",
    "  \n",
    "  # 예측값을 0 또는 1로 변환 (이진 분류)\n",
    "  predictions_binary = (torch.sigmoid(predictions) > 0.5).long().numpy()\n",
    "  \n",
    "  # 제출 파일 생성\n",
    "  submission_df = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': predictions_binary\n",
    "  })\n",
    "  \n",
    "  submission_df.to_csv(filename, index=False)\n",
    "  print(f\"✅ Predictions saved to {filename}\")\n",
    "  return submission_df\n",
    "\n",
    "def main(args):\n",
    "  current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "  config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'learning_rate': 1e-3,\n",
    "    'n_hidden_unit_list': [20, 20],\n",
    "    'activation_fn': args.activation_fn,\n",
    "    'patience': args.patience,\n",
    "  }\n",
    "\n",
    "  wandb.init(\n",
    "    mode=\"online\" if args.wandb else \"disabled\",\n",
    "    project=\"my_model_training\",\n",
    "    notes=\"My first wandb experiment\",\n",
    "    tags=[\"my_model\", \"Titanic\"],\n",
    "    # name=current_time_str,\n",
    "    name = args.name,\n",
    "    config=config\n",
    "  )\n",
    "  print(args)\n",
    "  print(wandb.config)\n",
    "\n",
    "  train_data_loader, validation_data_loader, test_data_loader = get_data()\n",
    "\n",
    "  linear_model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  training_loop(\n",
    "    model=linear_model,\n",
    "    optimizer=optimizer,\n",
    "    train_data_loader=train_data_loader,\n",
    "    validation_data_loader=validation_data_loader,\n",
    "    test_data_loader=test_data_loader\n",
    "  )\n",
    "  # Test 예측 및 CSV 저장\n",
    "  print(\"\\n\" + \"#\" * 50)\n",
    "  print(\"Generating predictions for test data...\")\n",
    "  test_predictions = predict_test(linear_model, test_data_loader)\n",
    "  \n",
    "  # 파일명에 run name 포함 (그래서 wandb에서 보이는 이름이 file name으로 저장되게 하는 코드임) -> ex) name: ReLU -> ReLU_submission.csv\n",
    "  # filename = f\"submission_{args.name if args.name else current_time_str}.csv\"\n",
    "  # 하단 코드는 submission으로만 만들어지게 함\n",
    "  save_predictions_to_csv(test_predictions)\n",
    "\n",
    "  wandb.finish()\n",
    "\n",
    "\n",
    "# https://docs.wandb.ai/guides/track/config\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.wandb = True  # wandb 사용 여부\n",
    "            self.batch_size = 128\n",
    "            self.epochs = 5000\n",
    "            self.name = \"ReLU_b128_early\"\n",
    "            self.activation_fn = \"ReLU\"\n",
    "            self.patience = 100\n",
    "\n",
    "    args = Args()\n",
    "    main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef1c4bb",
   "metadata": {},
   "source": [
    "# 각 셀별로 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ccdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didsu\\miniconda3\\envs\\link_dl\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didsu\\miniconda3\\envs\\link_dl\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: c:\\Users\\didsu\\workspace\\deeplearning\\link_dl\\_04_your_code\\homework_2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# 경로 설정\n",
    "BASE_PATH = os.getcwd()\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "from titanic_dataset import get_preprocessed_dataset\n",
    "\n",
    "# %%\n",
    "# 데이터 로드 함수\n",
    "def get_data(batch_size):\n",
    "    train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "    print(f\"Train: {len(train_dataset)}, Validation: {len(validation_dataset)}, Test: {len(test_dataset)}\")\n",
    "    \n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "    \n",
    "    return train_data_loader, validation_data_loader, test_data_loader\n",
    "\n",
    "# %%\n",
    "# 모델 정의\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden_units, activation_fn):\n",
    "        super().__init__()\n",
    "        # 활성화 함수 매핑\n",
    "        activation_functions = {\n",
    "            'ReLU': nn.ReLU(),\n",
    "            'Sigmoid': nn.Sigmoid(),\n",
    "            'ELU': nn.ELU(),\n",
    "            'Leaky ReLU': nn.LeakyReLU()\n",
    "        }\n",
    "        \n",
    "        # BCEWithLogitsLoss용 모델 (마지막에 활성화 함수 없음)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden_units[0]),\n",
    "            activation_functions[activation_fn],\n",
    "            nn.Linear(n_hidden_units[0], n_hidden_units[1]),\n",
    "            activation_functions[activation_fn],\n",
    "            nn.Linear(n_hidden_units[1], n_output),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# %%\n",
    "# 훈련 루프 함수\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader, \n",
    "                  n_epochs, patience, verbose=True):\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    next_print_epoch = 100\n",
    "    \n",
    "    # Early stopping 변수\n",
    "    best_validation_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Training\n",
    "        loss_train = 0.0\n",
    "        num_trains = 0\n",
    "        model.train()\n",
    "        \n",
    "        for train_batch in train_data_loader:\n",
    "            input_data = train_batch['input']\n",
    "            target = train_batch['target'].float().unsqueeze(1)\n",
    "            \n",
    "            output_train = model(input_data)\n",
    "            loss = loss_fn(output_train, target)\n",
    "            loss_train += loss.item()\n",
    "            num_trains += 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        loss_validation = 0.0\n",
    "        num_validations = 0\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for validation_batch in validation_data_loader:\n",
    "                input_data = validation_batch['input']\n",
    "                target = validation_batch['target'].float().unsqueeze(1)\n",
    "                \n",
    "                output_validation = model(input_data)\n",
    "                loss = loss_fn(output_validation, target)\n",
    "                loss_validation += loss.item()\n",
    "                num_validations += 1\n",
    "        \n",
    "        avg_train_loss = loss_train / num_trains\n",
    "        avg_validation_loss = loss_validation / num_validations\n",
    "        \n",
    "        # Wandb 로깅\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training loss\": avg_train_loss,\n",
    "            \"Validation loss\": avg_validation_loss\n",
    "        })\n",
    "        \n",
    "        # Early stopping 체크\n",
    "        if avg_validation_loss < best_validation_loss:\n",
    "            best_validation_loss = avg_validation_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # 주기적 출력\n",
    "        if verbose and epoch >= next_print_epoch:\n",
    "            print(f\"Epoch {epoch}, \"\n",
    "                  f\"Training loss {avg_train_loss:.4f}, \"\n",
    "                  f\"Validation loss {avg_validation_loss:.4f}, \"\n",
    "                  f\"Patience: {patience_counter}/{patience}\")\n",
    "            next_print_epoch += 100\n",
    "        \n",
    "        # Early stopping 조건\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "            print(f\"Best validation loss: {best_validation_loss:.4f}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "    \n",
    "    return model, best_validation_loss\n",
    "\n",
    "# %%\n",
    "# Test 예측 함수\n",
    "def predict_test(model, test_data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_data_loader:\n",
    "            input_data = test_batch['input']\n",
    "            output = model(input_data)\n",
    "            predictions.append(output)\n",
    "    \n",
    "    predictions = torch.cat(predictions, dim=0).squeeze()\n",
    "    return predictions\n",
    "\n",
    "# %%\n",
    "# CSV 저장 함수\n",
    "def save_predictions_to_csv(predictions, filename=\"submission.csv\"):\n",
    "    CURRENT_FILE_PATH = os.getcwd()\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    \n",
    "    # 예측값을 0 또는 1로 변환\n",
    "    predictions_binary = (torch.sigmoid(predictions) > 0.5).long().numpy()\n",
    "    \n",
    "    # 제출 파일 생성\n",
    "    submission_df = pd.DataFrame({\n",
    "        'PassengerId': test_df['PassengerId'],\n",
    "        'Survived': predictions_binary\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(f\"Predictions saved to {filename}\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2266b",
   "metadata": {},
   "source": [
    "# 요구사항 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99f21ee",
   "metadata": {},
   "source": [
    "_01_code/_08_learning_and_optimization/c_my_model_training_with_argparse_wandb.py 코드를 그대로 활용하되 titanic 데이터에 맞게 수정하여 코딩하기\n",
    "\n",
    "–Wandb로 훈련 과정 데이터올려 그래프얻어내기\n",
    "- Training loss\n",
    "- Validation loss\n",
    "- 위 두 그래프를 보여주는 WandbURL 얻어내기 : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06464bf",
   "metadata": {},
   "source": [
    "WandbURL 얻어내기 : https://wandb.ai/-ddj127-korea-university-of-technology-and-education/my_model_training?nw=nwuserddj127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f393337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: -ddj127 (-ddj127-korea-university-of-technology-and-education) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\didsu\\workspace\\deeplearning\\link_dl\\_04_your_code\\homework_2\\wandb\\run-20251017_203119-q8qsbmjf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2/runs/q8qsbmjf' target=\"_blank\">Req1_ReLU_b128_baseline</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2/runs/q8qsbmjf' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2/runs/q8qsbmjf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "요구사항 1: 기본 훈련 시작\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didsu\\workspace\\deeplearning\\link_dl\\_04_your_code\\homework_2\\titanic_dataset.py:129: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "c:\\Users\\didsu\\workspace\\deeplearning\\link_dl\\_04_your_code\\homework_2\\titanic_dataset.py:148: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
      "       'Embarked', 'title', 'family_num', 'alone'],\n",
      "      dtype='object')\n",
      "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  title  \\\n",
      "0       0.0       3    1  22.0      1      0   7.2500         2      2   \n",
      "1       1.0       1    0  38.0      1      0  71.2833         0      3   \n",
      "2       1.0       3    0  26.0      0      0   7.9250         2      1   \n",
      "3       1.0       1    0  35.0      1      0  53.1000         2      3   \n",
      "4       0.0       3    1  35.0      0      0   8.0500         2      2   \n",
      "5       0.0       3    1  29.0      0      0   8.4583         1      2   \n",
      "6       0.0       1    1  54.0      0      0  51.8625         2      2   \n",
      "7       0.0       3    1   2.0      3      1  21.0750         2      0   \n",
      "8       1.0       3    0  27.0      0      2  11.1333         2      3   \n",
      "9       1.0       2    0  14.0      1      0  30.0708         0      3   \n",
      "\n",
      "   family_num  alone  \n",
      "0           1    0.0  \n",
      "1           1    0.0  \n",
      "2           0    1.0  \n",
      "3           1    0.0  \n",
      "4           0    1.0  \n",
      "5           0    1.0  \n",
      "6           0    1.0  \n",
      "7           4    0.0  \n",
      "8           2    0.0  \n",
      "9           1    0.0  \n",
      "Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])\n",
      "Train: 713, Validation: 178, Test: 418\n",
      "Epoch 100, Training loss 0.6178, Validation loss 0.5652, Patience: 1/100\n",
      "Epoch 200, Training loss 0.5986, Validation loss 0.5612, Patience: 9/100\n",
      "Epoch 300, Training loss 0.5984, Validation loss 0.5607, Patience: 21/100\n",
      "Epoch 400, Training loss 0.5914, Validation loss 0.5589, Patience: 4/100\n",
      "Epoch 500, Training loss 0.5864, Validation loss 0.5581, Patience: 47/100\n",
      "Epoch 600, Training loss 0.5879, Validation loss 0.5557, Patience: 38/100\n",
      "Epoch 700, Training loss 0.5822, Validation loss 0.5554, Patience: 22/100\n",
      "Epoch 800, Training loss 0.5789, Validation loss 0.5548, Patience: 81/100\n",
      "Epoch 900, Training loss 0.5771, Validation loss 0.5527, Patience: 8/100\n",
      "Epoch 1000, Training loss 0.5688, Validation loss 0.5535, Patience: 6/100\n",
      "Epoch 1100, Training loss 0.5739, Validation loss 0.5503, Patience: 6/100\n",
      "Epoch 1200, Training loss 0.5625, Validation loss 0.5522, Patience: 25/100\n",
      "Epoch 1300, Training loss 0.5585, Validation loss 0.5486, Patience: 17/100\n",
      "Epoch 1400, Training loss 0.5604, Validation loss 0.5451, Patience: 5/100\n",
      "Epoch 1500, Training loss 0.5466, Validation loss 0.5431, Patience: 68/100\n",
      "Epoch 1600, Training loss 0.5484, Validation loss 0.5377, Patience: 37/100\n",
      "Epoch 1700, Training loss 0.5459, Validation loss 0.5348, Patience: 28/100\n",
      "\n",
      "Early stopping at epoch 1772\n",
      "Best validation loss: 0.5341\n",
      "\n",
      "요구사항 1 완료! Best Validation Loss: 0.5341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb URL: https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2/runs/q8qsbmjf\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>Training loss</td><td>█▄▃▃▃▃▃▃▃▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation loss</td><td>█▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1772</td></tr><tr><td>Training loss</td><td>0.54559</td></tr><tr><td>Validation loss</td><td>0.53467</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Req1_ReLU_b128_baseline</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2/runs/q8qsbmjf' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2/runs/q8qsbmjf</a><br> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251017_203119-q8qsbmjf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_req1 = {\n",
    "    'epochs': 5000,\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 1e-3,\n",
    "    'n_hidden_unit_list': [20, 20],\n",
    "    'activation_fn': 'ReLU',\n",
    "    'patience': 100,\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    mode=\"online\",\n",
    "    project=\"titanic_homework2\",\n",
    "    name=\"Req1_ReLU_b128_baseline\",\n",
    "    tags=[\"homework2\", \"requirement1\", \"baseline\"],\n",
    "    config=config_req1\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"요구사항 1: 기본 훈련 시작\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 데이터 로드\n",
    "train_loader, val_loader, test_loader = get_data(config_req1['batch_size'])\n",
    "\n",
    "# 모델 및 옵티마이저 생성\n",
    "model_req1 = MyModel(\n",
    "    n_input=10, \n",
    "    n_output=1, \n",
    "    n_hidden_units=config_req1['n_hidden_unit_list'],\n",
    "    activation_fn=config_req1['activation_fn']\n",
    ")\n",
    "optimizer_req1 = optim.SGD(model_req1.parameters(), lr=config_req1['learning_rate'])\n",
    "\n",
    "# 훈련\n",
    "model_req1, best_loss_req1 = training_loop(\n",
    "    model_req1, optimizer_req1, train_loader, val_loader,\n",
    "    config_req1['epochs'], config_req1['patience']\n",
    ")\n",
    "\n",
    "print(f\"\\n요구사항 1 완료! Best Validation Loss: {best_loss_req1:.4f}\")\n",
    "print(f\"Wandb URL: {wandb.run.get_url()}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c06e9",
   "metadata": {},
   "source": [
    "# 요구사항 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62452f",
   "metadata": {},
   "source": [
    "1. Activation Function 변경\n",
    "- 목표: 모델 구성 내에서 활성화 함수(Activation Function)를 변경하여 더 나은 성능을 내는 함수를 찾습니다.\n",
    "- 비교 대상: Sigmoid, ReLU, ELU, LeakyReLU\n",
    "\n",
    "2. Batch Size 변경\n",
    "- 목표: 모델 학습 시 배치 크기(Training Batch Size)를 변경하여 더 나은 성능을 내는 크기를 찾습니다.\n",
    "- 비교 대상: 16, 32, 64, 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e01a8",
   "metadata": {},
   "source": [
    "args의 값들을 변경하면서 진행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af14f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1800, Training loss 0.5524, Validation loss 0.5523, Patience: 1/100\n",
      "Epoch 1900, Training loss 0.5462, Validation loss 0.5485, Patience: 1/100\n",
      "Epoch 2000, Training loss 0.5415, Validation loss 0.5456, Patience: 6/100\n",
      "Epoch 2100, Training loss 0.5312, Validation loss 0.5418, Patience: 0/100\n",
      "Epoch 2200, Training loss 0.5390, Validation loss 0.5454, Patience: 3/100\n",
      "Epoch 2300, Training loss 0.5320, Validation loss 0.5378, Patience: 1/100\n",
      "Epoch 2400, Training loss 0.5224, Validation loss 0.5331, Patience: 9/100\n",
      "Epoch 2500, Training loss 0.5209, Validation loss 0.5294, Patience: 1/100\n",
      "Epoch 2600, Training loss 0.5160, Validation loss 0.5216, Patience: 4/100\n",
      "Epoch 2700, Training loss 0.5094, Validation loss 0.5165, Patience: 0/100\n",
      "Epoch 2800, Training loss 0.5010, Validation loss 0.5108, Patience: 1/100\n",
      "Epoch 2900, Training loss 0.5009, Validation loss 0.5259, Patience: 5/100\n",
      "Epoch 3000, Training loss 0.4921, Validation loss 0.5029, Patience: 4/100\n",
      "Epoch 3100, Training loss 0.4894, Validation loss 0.5094, Patience: 3/100\n",
      "Epoch 3200, Training loss 0.4818, Validation loss 0.4923, Patience: 9/100\n",
      "Epoch 3300, Training loss 0.4840, Validation loss 0.4942, Patience: 3/100\n",
      "Epoch 3400, Training loss 0.4730, Validation loss 0.4957, Patience: 3/100\n",
      "Epoch 3500, Training loss 0.4589, Validation loss 0.4768, Patience: 1/100\n",
      "Epoch 3600, Training loss 0.4559, Validation loss 0.4823, Patience: 1/100\n",
      "Epoch 3700, Training loss 0.4527, Validation loss 0.5081, Patience: 2/100\n",
      "Epoch 3800, Training loss 0.4410, Validation loss 0.4642, Patience: 0/100\n",
      "Epoch 3900, Training loss 0.4549, Validation loss 0.4647, Patience: 9/100\n",
      "Epoch 4000, Training loss 0.4377, Validation loss 0.4592, Patience: 5/100\n",
      "Epoch 4100, Training loss 0.4414, Validation loss 0.4561, Patience: 2/100\n",
      "Epoch 4200, Training loss 0.4441, Validation loss 0.5485, Patience: 34/100\n",
      "Epoch 4300, Training loss 0.4410, Validation loss 0.4662, Patience: 7/100\n",
      "Epoch 4400, Training loss 0.4399, Validation loss 0.4541, Patience: 9/100\n",
      "Epoch 4500, Training loss 0.4379, Validation loss 0.4498, Patience: 16/100\n",
      "Epoch 4600, Training loss 0.4341, Validation loss 0.4582, Patience: 16/100\n",
      "Epoch 4700, Training loss 0.4304, Validation loss 0.4493, Patience: 17/100\n",
      "Epoch 4800, Training loss 0.4622, Validation loss 0.4460, Patience: 62/100\n",
      "Epoch 4900, Training loss 0.4394, Validation loss 0.4581, Patience: 31/100\n",
      "Epoch 5000, Training loss 0.4297, Validation loss 0.4543, Patience: 4/100\n",
      "\n",
      "요구사항 2 완료! Best Validation Loss: 0.4395\n",
      "Wandb URL: https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2/runs/puqzu1vq\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>Training loss</td><td>█▇▇▆▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▁</td></tr><tr><td>Validation loss</td><td>██▇▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▂▁▁▁▁▄▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5000</td></tr><tr><td>Training loss</td><td>0.42969</td></tr><tr><td>Validation loss</td><td>0.45434</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Req2_ReLU_b128</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2/runs/puqzu1vq' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2/runs/puqzu1vq</a><br> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/titanic_homework2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251017_204937-puqzu1vq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BEST_ACTIVATION = 'ReLU'  # 'Sigmoid', 'ReLU', 'ELU', 'Leaky ReLU' 중 선택\n",
    "BEST_BATCH_SIZE = 128     # 16, 32, 64, 128 중 선택\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"요구사항 2: 선택한 Activation Function & Batch Size로 훈련\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"선택한 조합:\")\n",
    "print(f\"  - Activation Function: {BEST_ACTIVATION}\")\n",
    "print(f\"  - Batch Size: {BEST_BATCH_SIZE}\")\n",
    "\n",
    "config_req2 = {\n",
    "    'epochs': 5000,\n",
    "    'batch_size': BEST_BATCH_SIZE,\n",
    "    'learning_rate': 1e-3,\n",
    "    'n_hidden_unit_list': [20, 20],\n",
    "    'activation_fn': BEST_ACTIVATION,\n",
    "    'patience': 100,\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    mode=\"online\",\n",
    "    project=\"titanic_homework2\",\n",
    "    name=f\"Req2_{BEST_ACTIVATION}_b{BEST_BATCH_SIZE}\",\n",
    "    tags=[\"homework2\", \"requirement2\", BEST_ACTIVATION, f\"batch_{BEST_BATCH_SIZE}\"],\n",
    "    config=config_req2\n",
    ")\n",
    "\n",
    "# 데이터 로드\n",
    "train_loader, val_loader, test_loader = get_data(BEST_BATCH_SIZE)\n",
    "\n",
    "# 모델 및 옵티마이저 생성\n",
    "model_req2 = MyModel(\n",
    "    n_input=10, \n",
    "    n_output=1, \n",
    "    n_hidden_units=config_req2['n_hidden_unit_list'],\n",
    "    activation_fn=BEST_ACTIVATION\n",
    ")\n",
    "optimizer_req2 = optim.SGD(model_req2.parameters(), lr=config_req2['learning_rate'])\n",
    "\n",
    "# 훈련\n",
    "model_req2, best_loss_req2 = training_loop(\n",
    "    model_req2, optimizer_req2, train_loader, val_loader,\n",
    "    config_req2['epochs'], config_req2['patience']\n",
    ")\n",
    "\n",
    "print(f\"\\n요구사항 2 완료! Best Validation Loss: {best_loss_req2:.4f}\")\n",
    "print(f\"Wandb URL: {wandb.run.get_url()}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad91435",
   "metadata": {},
   "source": [
    "# 요구사항 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e48cb",
   "metadata": {},
   "source": [
    "모델 구성: '요구사항 2'에서 찾은 가장 좋은 성능의 **활성화 함수(Activation Function)**와 **배치 크기(Batch Size)**를 사용하여 모델을 구성합니다.\n",
    "\n",
    "데이터 준비: 사전에 테스트 데이터(test_data_loader)를 구성합니다.\n",
    "\n",
    "테스트 시점 고찰: 훈련 과정 중 어느 에포크(Epoch) 시점에서 테스트를 수행해야 submission.csv 파일을 가장 효과적으로 생성할 수 있을지 고찰합니다.\n",
    "\n",
    "추가 코딩: 위 고찰 내용을 바탕으로 필요한 코드를 추가로 구현합니다.\n",
    "\n",
    "결과 제출: 최종적으로 submission.csv 파일을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb536bbc",
   "metadata": {},
   "source": [
    "| 추가 코딩 구현했음\n",
    "활성화 함수: ReLU\n",
    "- 요구사항 2에서 다양한 활성화 함수(ReLU, Sigmoid, ELU, Leaky ReLU)를 비교 실험한 결과, ReLU가 가장 우수한 성능을 보임\n",
    "- 선택 이유: 학습 속도가 빠르고 gradient vanishing 문제가 적으며, validation loss가 가장 낮게 수렴\n",
    "\n",
    "\n",
    "배치 크기: 128\n",
    "- 요구사항 2에서 여러 배치 크기(32, 64, 128, 256)를 실험한 결과, 128이 최적\n",
    "- 선택 이유: 학습 안정성과 속도의 균형이 좋고, 메모리 효율적이면서도 gradient 업데이트가 안정적\n",
    "\n",
    "csv 파일로 저장하는 코드는\n",
    "- save_predictions_to_csv(...)로 진행함.\n",
    "predict_test()에서 모든 배치의 예측값(logits)을 모은 다음, save_predictions_to_csv()에서 한 번에 sigmoid를 적용하고 이진화함\n",
    "\n",
    "early stoping의 기준을 잡음.\n",
    "```class Args:\n",
    "    def __init__(self):\n",
    "        ....\n",
    "        self.patience = 100```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1212326d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "요구사항 3: Test 데이터 예측 및 Submission 파일 생성\n",
      "============================================================\n",
      "✅ Predictions saved to submission.csv\n",
      "\n",
      " submission.csv 파일이 생성되었습니다!\n",
      "   사용된 모델: ReLU + Batch Size 128\n",
      "   Best Validation Loss: 0.4395\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"요구사항 3: Test 데이터 예측 및 Submission 파일 생성\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 요구사항 2에서 훈련한 model_req2를 그대로 사용\n",
    "test_predictions = predict_test(model_req2, test_loader)\n",
    "submission_df = save_predictions_to_csv(test_predictions, \"submission.csv\")\n",
    "\n",
    "print(f\"\\n submission.csv 파일이 생성되었습니다!\")\n",
    "print(f\"   사용된 모델: {BEST_ACTIVATION} + Batch Size {BEST_BATCH_SIZE}\")\n",
    "print(f\"   Best Validation Loss: {best_loss_req2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667aebf",
   "metadata": {},
   "source": [
    "# 요구사항 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2899a",
   "metadata": {},
   "source": [
    "1. submission.csv 파일 제출\n",
    "\n",
    "Kaggle에 로그인한 후, \"Submit Prediction\" 기능을 사용하여 submission.csv 파일을 제출합니다.\n",
    "\n",
    "2. Leaderboard 순위 확인 및 캡처\n",
    "\n",
    "Leaderboard에 등록된 자신의 점수와 순위가 나오도록 스크린 이미지를 캡처합니다.\n",
    "\n",
    "3. Jupyter Notebook에 이미지 추가\n",
    "\n",
    "캡처한 이미지를 클라우드에 업로드하여 URL을 생성합니다.\n",
    "\n",
    "생성된 URL을 사용하여 이미지를 Jupyter Notebook에 넣습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fc51b",
   "metadata": {},
   "source": [
    "<img src=\"submission.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a374015d",
   "metadata": {},
   "source": [
    "<img src=\"submission_scoreAndRank.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766889b",
   "metadata": {},
   "source": [
    "# 숙제 후기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8744c2",
   "metadata": {},
   "source": [
    "1. 전체적인 진행방식\n",
    " - 16, 32, 64, 128 batch size와 epoch는 1000으로 loss의 변화 추이를 그래프로 파악. 이후 선별된 소수의 activation & batchsize를 통해 2000 epoch까지 진행. 최종 선별된 명명된  \n",
    " ReLU_b128을 통해서 5000과 10000까지 진행함.\n",
    " \n",
    " - ReLU_b128가 5xxx까지는 괜찮다가, 10000까지 진행된 상황을 보면, 그래프 선 추이가 많이 Flucation이 발생함을 확인함. 본인은 해당 문제를 과적합의 문제라고도 봄. 하여, 추가적인 조치가 필요하다는 결론을 내림.\n",
    "\n",
    " - 생각보다 activation과 batch만으로는 loss값의 하락이 쉽지가 않았음.\n",
    "\n",
    "2. XGBBoost를 사용하기도 함.\n",
    " - model이라는 개념이 필요하지 않았다라는 점이 생소하기도 했고, 뭔가 다른 점들이 많아서 헷갈리는 부분도 많았음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9baa2d",
   "metadata": {},
   "source": [
    "## 학습 과정의 그래프 모음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1902309",
   "metadata": {},
   "source": [
    "16, 32, 64, 128 batch size와 epoch는 1000으로 다양한 activations들의 loss의 변화 추이를 그래프로 파악."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3515ac",
   "metadata": {},
   "source": [
    "<img src=\"Multiples And All Activation Functions.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76580ea1",
   "metadata": {},
   "source": [
    "하나의 예시로 ReLU_b64의 경우에 Validation loss의 경우에 위로 솟음이 보이는데 본인은 과적합의 현상으로 보여 최종 모델을 선별하기 위해서 이러한 추이가 보이는 것들은 제거함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b2fbb",
   "metadata": {},
   "source": [
    "<img src=\"위로는 올라가지만 아래로는 안내려가서 비교안하기.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666872ce",
   "metadata": {},
   "source": [
    "하단은 제거 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9790593",
   "metadata": {},
   "source": [
    "<img src=\"3개 선정 이후 다시 에포크 늘리기.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807a144",
   "metadata": {},
   "source": [
    "이제 각각을 2000 에포크로 늘려봤음.\n",
    "\n",
    "근데 ReLU batch 32은 과적합 신호로 인해서, Leaky ReLU batch 128의 경우엔 줄어드는 추이가 낮아서 제거함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8237443",
   "metadata": {},
   "source": [
    "<img src=\"3개 선정 이후 1500epoch 결과.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad01689",
   "metadata": {},
   "source": [
    "ReLU batch 128을 10000까지 에포크를 늘려봄. \n",
    "\n",
    "4K 에포크에서 과적합의 초기 추세가 보임.\n",
    "\n",
    "4xxx epoch가 최종적으로 가장 적합한 모델의 상태라는 결론에 도달함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d372d48",
   "metadata": {},
   "source": [
    "<img src=\"ReLU_batch128을 epoch10000까지 한 결과.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3f6b8",
   "metadata": {},
   "source": [
    "최종 결과( 5000 epoch로 다시 돌림)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2075d37f",
   "metadata": {},
   "source": [
    "<img src=\"ReLU_batch128 최종적으로 결과 모음.png\" width=\"1000\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "link_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
